{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install lightgbm\n",
    "# !pip install sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/tf/notebooks/19th_problem/20_forecast_proton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proton = pd.read_csv(\"train/train_proton.csv\")\n",
    "test_proton = pd.read_csv(\"test/test_proton.csv\")\n",
    "# test_proton = pd.read_csv(\"val/val_proton.csv\")\n",
    "\n",
    "train_AC_H1_EPM = pd.read_csv(\"train/train_AC_H1_EPM.csv\")\n",
    "test_AC_H1_EPM = pd.read_csv(\"test/test_AC_H1_EPM.csv\")\n",
    "# test_AC_H1_EPM = pd.read_csv(\"val/val_AC_H1_EPM.csv\")\n",
    "\n",
    "train_xray = pd.read_csv(\"train/train_xray.csv\")\n",
    "test_xray = pd.read_csv(\"test/test_xray.csv\")\n",
    "# test_xray = pd.read_csv(\"val/val_xray.csv\")\n",
    "\n",
    "train_AC_H0_SWE = pd.read_csv(\"train/train_AC_H0_SWE.csv\")\n",
    "test_AC_H0_SWE = pd.read_csv(\"test/test_AC_H0_SWE.csv\")\n",
    "# test_AC_H0_SWE = pd.read_csv(\"val/val_AC_H0_SWE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xray['ratio'] = (train_xray['xl']/train_xray['xs'])\n",
    "# train_xray['r2_plus'] = (train_xray['xl']**2+train_xray['xs']**2)\n",
    "\n",
    "# test_xray['ratio'] = (test_xray['xl']/test_xray['xs'])\n",
    "# test_xray['r2_plus'] = (test_xray['xl']**2+test_xray['xs']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataframes in [train_proton,test_proton,train_AC_H1_EPM,test_AC_H1_EPM,train_xray,test_xray,train_AC_H0_SWE,test_AC_H0_SWE]:\n",
    "#     for column in dataframes.columns:\n",
    "#         dataframes[column] = np.where(dataframes[column]==-100,0,dataframes[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_AC_H1_EPM['time_tag'] = pd.to_datetime(train_AC_H1_EPM['time_tag'].str.slice(0,-1))\n",
    "\n",
    "train_AC_H0_SWE['time_tag'] = pd.to_datetime(train_AC_H0_SWE['time_tag'].str.slice(0,-1))\n",
    "\n",
    "train_proton['time_tag'] = pd.to_datetime(train_proton['time_tag'])\n",
    "\n",
    "train_xray['time_tag'] = pd.to_datetime(train_xray['time_tag'])\n",
    "\n",
    "test_AC_H1_EPM['time_tag'] = pd.to_datetime(test_AC_H1_EPM['time_tag'].str.slice(0,-1))\n",
    "\n",
    "test_AC_H0_SWE['time_tag'] = pd.to_datetime(test_AC_H0_SWE['time_tag'].str.slice(0,-1))\n",
    "\n",
    "test_proton['time_tag'] = pd.to_datetime(test_proton['time_tag'])\n",
    "\n",
    "test_xray['time_tag'] = pd.to_datetime(test_xray['time_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_AC_H1_EPM['time_tag'] = pd.to_datetime(train_AC_H1_EPM['time_tag'], unit='s')\n",
    "\n",
    "train_AC_H0_SWE['time_tag'] = pd.to_datetime(train_AC_H0_SWE['time_tag'], unit='s')\n",
    "\n",
    "train_proton['time_tag'] = pd.to_datetime(train_proton['time_tag'], unit='s')\n",
    "\n",
    "train_xray['time_tag'] = pd.to_datetime(train_xray['time_tag'], unit='s')\n",
    "\n",
    "test_AC_H1_EPM['time_tag'] = pd.to_datetime(test_AC_H1_EPM['time_tag'], unit='s')\n",
    "\n",
    "test_AC_H0_SWE['time_tag'] = pd.to_datetime(test_AC_H0_SWE['time_tag'], unit='s')\n",
    "\n",
    "test_proton['time_tag'] = pd.to_datetime(test_proton['time_tag'], unit='s')\n",
    "\n",
    "test_xray['time_tag'] = pd.to_datetime(test_xray['time_tag'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_proton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_AC_H1_EPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_AC_H1_EPM = train_AC_H1_EPM.reset_index().set_index('time_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)']:\n",
    "    if column=='P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)':\n",
    "        lens = train_AC_H1_EPM[column].resample('5T').size().reset_index(name=column+'_lens')\n",
    "        mean = train_AC_H1_EPM[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#         sums = train_AC_H1_EPM[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "        t = pd.merge(lens,mean,on=['time_tag'],how='left')\n",
    "#         t = pd.merge(t,sums,on=['time_tag'],how='left')\n",
    "    else:\n",
    "        mean = train_AC_H1_EPM[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#         sums = train_AC_H1_EPM[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "#         t = pd.merge(mean,sums,on=['time_tag'],how='left')\n",
    "        t = mean.copy()\n",
    "\n",
    "    if column=='P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)':\n",
    "        final_AC_H1_EPM = t\n",
    "    else:\n",
    "        final_AC_H1_EPM = pd.merge(final_AC_H1_EPM,t,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train = pd.merge(train_proton,final_AC_H1_EPM,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_AC_H1_EPM = test_AC_H1_EPM.reset_index().set_index('time_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)',\n",
    "       'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)']:\n",
    "    if column=='P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)':\n",
    "        lens = test_AC_H1_EPM[column].resample('5T').size().reset_index(name=column+'_lens')\n",
    "        mean = test_AC_H1_EPM[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#         sums = test_AC_H1_EPM[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "        t = pd.merge(lens,mean,on=['time_tag'],how='left')\n",
    "#         t = pd.merge(t,sums,on=['time_tag'],how='left')\n",
    "    else:\n",
    "        mean = test_AC_H1_EPM[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#         sums = test_AC_H1_EPM[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "#         t = pd.merge(mean,sums,on=['time_tag'],how='left')\n",
    "        t = mean.copy()\n",
    "\n",
    "    if column=='P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)':\n",
    "        final_AC_H1_EPM = t\n",
    "    else:\n",
    "        final_AC_H1_EPM = pd.merge(final_AC_H1_EPM,t,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = pd.merge(test_proton,final_AC_H1_EPM,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_xray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_tag', 'xs', 'xl'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xray.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xray = train_xray.reset_index().set_index('time_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [x for x in train_xray.columns if x!='index']:\n",
    "    mean = train_xray[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#     sums = train_xray[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "#     t = pd.merge(mean,sums,on=['time_tag'],how='left')\n",
    "    t = mean.copy()\n",
    "\n",
    "    if column=='xs':\n",
    "        final_xray = t\n",
    "    else:\n",
    "        final_xray = pd.merge(final_xray,t,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train = pd.merge(real_train,final_xray,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xray = test_xray.reset_index().set_index('time_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [x for x in train_xray.columns if x!='index']:\n",
    "#     sums = test_xray[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "    mean = test_xray[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#     t = pd.merge(mean,sums,on=['time_tag'],how='left')\n",
    "    t = mean.copy()\n",
    "\n",
    "    if column=='xs':\n",
    "        final_xray = t\n",
    "    else:\n",
    "        final_xray = pd.merge(final_xray,t,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = pd.merge(real_test,final_xray,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_AC_H0_SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_AC_H0_SWE = train_AC_H0_SWE.reset_index().set_index('time_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in ['H_DENSITY_#/cc','SW_H_SPEED_km/s']:\n",
    "    if column=='H_DENSITY_#/cc':\n",
    "        lens = train_AC_H0_SWE[column].resample('5T').size().reset_index(name=column+'_lens')\n",
    "#         sums = train_AC_H0_SWE[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "        mean = train_AC_H0_SWE[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "        t = pd.merge(lens,mean,on=['time_tag'],how='left')\n",
    "#         t = pd.merge(t,sums,on=['time_tag'],how='left')\n",
    "    else:\n",
    "#         sums = train_AC_H0_SWE[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "        mean = train_AC_H0_SWE[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#         t = pd.merge(mean,sums,on=['time_tag'],how='left')\n",
    "        t = mean.copy()\n",
    "\n",
    "    if column=='H_DENSITY_#/cc':\n",
    "        final_AC_H0_SWE = t\n",
    "    else:\n",
    "        final_AC_H0_SWE = pd.merge(final_AC_H0_SWE,t,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train = pd.merge(real_train,final_AC_H0_SWE,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_AC_H0_SWE = test_AC_H0_SWE.reset_index().set_index('time_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in ['H_DENSITY_#/cc','SW_H_SPEED_km/s']:\n",
    "    if column=='H_DENSITY_#/cc':\n",
    "        lens = test_AC_H0_SWE[column].resample('5T').size().reset_index(name=column+'_lens')\n",
    "#         sums = test_AC_H0_SWE[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "        mean = test_AC_H0_SWE[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "        t = pd.merge(lens,mean,on=['time_tag'],how='left')\n",
    "#         t = pd.merge(t,sums,on=['time_tag'],how='left')\n",
    "    else:\n",
    "#         sums = test_AC_H0_SWE[column].resample('5T').sum().reset_index(name=column+'_sum')\n",
    "        mean = test_AC_H0_SWE[column].resample('5T').mean().reset_index(name=column+'_mean')\n",
    "#         t = pd.merge(mean,sums,on=['time_tag'],how='left')\n",
    "        t = mean.copy()\n",
    "\n",
    "    if column=='H_DENSITY_#/cc':\n",
    "        final_AC_H0_SWE = t\n",
    "    else:\n",
    "        final_AC_H0_SWE = pd.merge(final_AC_H0_SWE,t,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = pd.merge(real_test,final_AC_H0_SWE,on=['time_tag'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lag 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mustin_columns = [x for x in real_train.columns if x not in ['time_tag','proton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag_list=[1,2,3,4,5,288,576]\n",
    "lag_list = [1,288,2,-288]\n",
    "# lag_list = lag_list+[-x for x in lag_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in lag_list:\n",
    "\n",
    "    real_train = real_train.set_index('time_tag')\n",
    "\n",
    "    column_list = [x for x in real_train.columns if (x not in ['proton','time']) and ('_lag' not in x)]\n",
    "\n",
    "    shift_df = real_train[column_list].shift(lag)\n",
    "\n",
    "    for i in [{x:x+'_lag'+str(lag)} for x in column_list]:\n",
    "        shift_df = shift_df.rename(columns=i)\n",
    "\n",
    "    real_train = pd.merge(real_train,shift_df.reset_index(),on='time_tag',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in lag_list:\n",
    "\n",
    "    real_test = real_test.set_index('time_tag')\n",
    "\n",
    "    column_list = [x for x in real_test.columns if (x not in ['proton','time']) and ('_lag' not in x)]\n",
    "\n",
    "    shift_df = real_test[column_list].shift(lag)\n",
    "\n",
    "    for i in [{x:x+'_lag'+str(lag)} for x in column_list]:\n",
    "        shift_df = shift_df.rename(columns=i)\n",
    "\n",
    "    real_test = pd.merge(real_test,shift_df.reset_index(),on='time_tag',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train = real_train.loc[~real_train['proton'].isnull()].reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = real_train['time_tag'].dt.time.astype(str)\n",
    "real_train['time'] = time.str.split(':',expand=True)[0].astype(int)*60+time.str.split(':',expand=True)[1].astype(int)\n",
    "\n",
    "time = real_test['time_tag'].dt.time.astype(str)\n",
    "real_test['time'] = time.str.split(':',expand=True)[0].astype(int)*60+time.str.split(':',expand=True)[1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_proton\n",
    "del train_AC_H1_EPM\n",
    "del test_AC_H1_EPM\n",
    "del train_xray\n",
    "del test_xray\n",
    "del train_AC_H0_SWE\n",
    "del test_AC_H0_SWE\n",
    "del shift_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필수 컬럼과 논필수 컬럼 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mustin_columns = mustin_columns+[x for x in real_train.columns if 'lag1' in x]+['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag2_columns = [x for x in real_train.columns if ('lag2' in x) and ('lag288' not in x)]\n",
    "lag288_columns = [x for x in real_train.columns if ('lag288' in x)]\n",
    "lag_288_columns = [x for x in real_train.columns if ('lag-288' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_option = [lag2_columns,lag288_columns,lag_288_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_eval(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    y = np.where(y<0,0,y)\n",
    "    \n",
    "    y_hat = pd.Series(y_hat)\n",
    "    y_hat = np.where(y_hat<0,0,y_hat)\n",
    "\n",
    "    return 'myloss', np.mean((np.log(y_hat+1)-np.log(y+1))**2), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    y = np.where(y<0,0,y)\n",
    "    \n",
    "    y_hat = pd.Series(y_hat)\n",
    "    y_hat = np.where(y_hat<0,0,y_hat)\n",
    "    grad = (1/(y_hat+1))*(np.log(y_hat+1)-np.log(y+1))\n",
    "    hess = (1/(y_hat+1))**2-(np.log(y_hat+1)-np.log(y+1))/(y_hat+1)**2\n",
    "    \n",
    "#     grad = pd.Series(grad)\n",
    "#     hess = pd.Series(hess)\n",
    "\n",
    "#     grad = np.where(y<10,grad,np.where(y<100,grad*100,np.where(y<1000,grad*200,np.where(y<10000,grad*1000,grad*6000))))\n",
    "#     hess = np.where(y<10,hess,np.where(y<100,hess*100,np.where(y<1000,hess*200,np.where(y<10000,hess*1000,hess*6000))))\n",
    "\n",
    "    return grad,hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof_lgbm(params, train_data, test_data, target_data, num_round, early_round, verbose_round, N_SPLITS=5, random_state=0):\n",
    "\n",
    "    FOLDs=KFold(n_splits=N_SPLITS, shuffle=True,random_state=0)\n",
    "\n",
    "    oof = np.zeros(len(train_data))\n",
    "    predictions = np.zeros(len(test_data))\n",
    "\n",
    "#     features_lgb = list(train_data.columns)\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    iter_list=[]\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train_data)):\n",
    "        trn_data = lgb.Dataset(train_data.iloc[trn_idx], label=target_data.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train_data.iloc[val_idx], label=target_data.iloc[val_idx])\n",
    "\n",
    "        print(\"LGB \" + str(fold_) + \"-\" * 50)\n",
    "        num_round = num_round\n",
    "        clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=verbose_round,\n",
    "                        fobj=rmsle,\n",
    "                        feval = rmsle_eval,\n",
    "                        early_stopping_rounds = early_round)\n",
    "        oof[val_idx] = clf.predict(train_data.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "        iter_list.append(clf.best_iteration)\n",
    "#         fold_importance_df = pd.DataFrame()\n",
    "#         fold_importance_df[\"feature\"] = features_lgb\n",
    "#         fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#         fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        predictions += clf.predict(test_data, num_iteration=clf.best_iteration) / FOLDs.n_splits\n",
    "        joblib.dump(clf,'/tf/notebooks/19th_problem/lightgbm_model/lgb'+str(fold_)+'.pkl')\n",
    "    return oof, predictions, np.mean(iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "xgb_params={\"objective\":\"regression\",\n",
    "           \"metric\":\"myloss\",\n",
    "           \"max_depth\":6,\n",
    "           \"min_child_samples\":2,\n",
    "           \"alpha\":0.08,\n",
    "           \"gamma\":0.06,\n",
    "           \"eta\":0.04,\n",
    "           \"subsample\":0.08,\n",
    "           \"colsample_bytree\":0.97,\n",
    "           \"random_state\":2020\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train={}\n",
    "agg_test={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) original_변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) 변수선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB 0--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.125593\tvalid_1's myloss: 0.130477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f6fe4cb7dd35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proton'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_oof_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_SPLITS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrn_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-caec6511a7b1>\u001b[0m in \u001b[0;36mget_oof_lgbm\u001b[0;34m(params, train_data, test_data, target_data, num_round, early_round, verbose_round, N_SPLITS, random_state)\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmsle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmsle_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                         early_stopping_rounds = early_round)\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0miter_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1981\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1983\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__boost\u001b[0;34m(self, grad, hess)\u001b[0m\n\u001b[1;32m   2016\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m             \u001b[0mhess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2018\u001b[0;31m             ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   2019\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(columns_option)):\n",
    "    selected_var = [x for x in columns_option if x!=columns_option[i]]\n",
    "    selected_var = selected_var[0]+selected_var[1]\n",
    "    X_train = real_train[mustin_columns+selected_var]\n",
    "    X_test = real_test[mustin_columns+selected_var]\n",
    "    y_train = real_train['proton']\n",
    "\n",
    "    a,b,c=get_oof_lgbm(xgb_params, X_train, X_test, y_train, num_round=100000, early_round=400, verbose_round=500, N_SPLITS=5, random_state=0)\n",
    "\n",
    "    trn_data = lgb.Dataset(X_train.iloc[:500000], label=y_train.iloc[:500000])\n",
    "    val_data = lgb.Dataset(X_train.iloc[500000:], label=y_train.iloc[500000:])\n",
    "\n",
    "    clf = lgb.train(xgb_params, trn_data, int(c),fobj=rmsle,feval = rmsle_eval)\n",
    "    agg_test[i] = b\n",
    "    agg_train[i] = clf.predict(X_train.iloc[500000:], num_iteration=clf.best_iteration)\n",
    "    print(np.mean(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking = pd.DataFrame({'featureset_1':agg_train[0],\n",
    "              'featureset_2':agg_train[1],\n",
    "              'featureset_3':agg_train[2],\n",
    "              'proton':y_train.iloc[500000:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_stacking = pd.DataFrame({'featureset_1':agg_test[0],\n",
    "              'featureset_2':agg_test[1],\n",
    "              'featureset_3':agg_test[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_train.csv\",index=False)\n",
    "ts_stacking.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tr_stacking\n",
    "del ts_stacking\n",
    "del agg_test\n",
    "del agg_train\n",
    "del a\n",
    "del b\n",
    "del c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mustin_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-739feba8aa9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcolumn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmustin_columns\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlag2_columns\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlag288_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mustin_columns' is not defined"
     ]
    }
   ],
   "source": [
    "column_list = mustin_columns+lag2_columns+lag288_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = real_train[column_list]\n",
    "X_test = real_test[column_list]\n",
    "y_train = real_train['proton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\ttraining's myloss: 0.114835\tvalid_1's myloss: 0.172316\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttraining's myloss: 0.158742\tvalid_1's myloss: 0.142359\n"
     ]
    }
   ],
   "source": [
    "trn_data = lgb.Dataset(X_train.iloc[:500000], label=y_train.iloc[:500000])\n",
    "val_data = lgb.Dataset(X_train.iloc[500000:], label=y_train.iloc[500000:])\n",
    "\n",
    "clf = lgb.train(xgb_params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=500,\n",
    "                fobj=rmsle,\n",
    "                feval = rmsle_eval,\n",
    "                early_stopping_rounds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_eval1(y_hat, y):\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    y = np.where(y<0,0,y)\n",
    "    \n",
    "    y_hat = pd.Series(y_hat)\n",
    "    y_hat = np.where(y_hat<0,0,y_hat)\n",
    "\n",
    "    return 'myloss', np.mean((np.log(y_hat+1)-np.log(y+1))**2), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "xs_mean\n",
      "xl_mean\n",
      "H_DENSITY_#/cc_lens\n",
      "H_DENSITY_#/cc_mean\n",
      "SW_H_SPEED_km/s_mean\n",
      "time\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag1\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "xs_mean_lag1\n",
      "xl_mean_lag1\n",
      "H_DENSITY_#/cc_lens_lag1\n",
      "H_DENSITY_#/cc_mean_lag1\n",
      "SW_H_SPEED_km/s_mean_lag1\n",
      "time_lag1\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag2\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "xs_mean_lag2\n",
      "xl_mean_lag2\n",
      "H_DENSITY_#/cc_lens_lag2\n",
      "H_DENSITY_#/cc_mean_lag2\n",
      "SW_H_SPEED_km/s_mean_lag2\n",
      "time_lag2\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag288\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "xs_mean_lag288\n",
      "xl_mean_lag288\n",
      "H_DENSITY_#/cc_lens_lag288\n",
      "H_DENSITY_#/cc_mean_lag288\n",
      "SW_H_SPEED_km/s_mean_lag288\n",
      "time_lag288\n"
     ]
    }
   ],
   "source": [
    "rmsle_list=[]\n",
    "for column in X_train.columns:\n",
    "    X_train_fake = X_train.iloc[500000:].copy()\n",
    "    X_train_fake[column] = X_train_fake[column].sample(len(X_train_fake),replace=False,random_state=2020).reset_index(drop=True)\n",
    "    rmsle_list.append(rmsle_eval1(clf.predict(X_train_fake, num_iteration=clf.best_iteration),y_train.iloc[500000:]))\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.14235926700979437, False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fake = X_train.iloc[500000:].copy()\n",
    "rmsle_eval1(clf.predict(X_train_fake, num_iteration=clf.best_iteration),y_train.iloc[500000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame({'rmsle':[x[1] for x in rmsle_list],'col_list':X_train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp['rmsle'] = imp['rmsle']-0.14235926700979437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
       " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
       " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
       " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
       " 'xs_mean',\n",
       " 'H_DENSITY_#/cc_mean',\n",
       " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
       " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
       " 'xs_mean_lag1',\n",
       " 'xl_mean_lag1',\n",
       " 'H_DENSITY_#/cc_mean_lag1',\n",
       " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
       " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
       " 'xs_mean_lag2',\n",
       " 'time_lag2',\n",
       " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
       " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
       " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
       " 'xs_mean_lag288',\n",
       " 'H_DENSITY_#/cc_mean_lag288',\n",
       " 'time_lag288']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.loc[imp['rmsle']<0]['col_list'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [x for x in column_list if x not in imp.loc[imp['rmsle']<0]['col_list'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = real_train[column_list]\n",
    "X_test = real_test[column_list]\n",
    "\n",
    "y_train = real_train['proton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB 0--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.134333\tvalid_1's myloss: 0.135307\n",
      "[1000]\ttraining's myloss: 0.120879\tvalid_1's myloss: 0.125057\n",
      "[1500]\ttraining's myloss: 0.117508\tvalid_1's myloss: 0.124623\n",
      "Early stopping, best iteration is:\n",
      "[1247]\ttraining's myloss: 0.1176\tvalid_1's myloss: 0.123214\n",
      "LGB 1--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.134941\tvalid_1's myloss: 0.136919\n",
      "[1000]\ttraining's myloss: 0.119466\tvalid_1's myloss: 0.125774\n",
      "[1500]\ttraining's myloss: 0.115009\tvalid_1's myloss: 0.123234\n",
      "[2000]\ttraining's myloss: 0.11052\tvalid_1's myloss: 0.120745\n",
      "Early stopping, best iteration is:\n",
      "[2087]\ttraining's myloss: 0.110024\tvalid_1's myloss: 0.12033\n",
      "LGB 2--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.136706\tvalid_1's myloss: 0.144867\n",
      "[1000]\ttraining's myloss: 0.120706\tvalid_1's myloss: 0.130945\n",
      "[1500]\ttraining's myloss: 0.11878\tvalid_1's myloss: 0.132117\n",
      "Early stopping, best iteration is:\n",
      "[1112]\ttraining's myloss: 0.118366\tvalid_1's myloss: 0.129447\n",
      "LGB 3--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.130392\tvalid_1's myloss: 0.132332\n",
      "[1000]\ttraining's myloss: 0.112865\tvalid_1's myloss: 0.118269\n",
      "Early stopping, best iteration is:\n",
      "[921]\ttraining's myloss: 0.110906\tvalid_1's myloss: 0.114644\n",
      "LGB 4--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.138647\tvalid_1's myloss: 0.14436\n",
      "[1000]\ttraining's myloss: 0.133205\tvalid_1's myloss: 0.140941\n",
      "[1500]\ttraining's myloss: 0.128717\tvalid_1's myloss: 0.138985\n",
      "Early stopping, best iteration is:\n",
      "[1468]\ttraining's myloss: 0.128694\tvalid_1's myloss: 0.138911\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "xgb_params={\"objective\":\"regression\",\n",
    "           \"metric\":\"myloss\",\n",
    "           \"max_depth\":6,\n",
    "           \"min_child_samples\":2,\n",
    "           \"alpha\":0.08,\n",
    "           \"gamma\":0.06,\n",
    "           \"eta\":0.04,\n",
    "           \"subsample\":0.08,\n",
    "           \"colsample_bytree\":0.97,\n",
    "           \"random_state\":2020\n",
    "           }\n",
    "a,b,c=get_oof_lgbm(xgb_params, X_train, X_test, y_train, num_round=100000, early_round=400, verbose_round=500, N_SPLITS=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking = pd.read_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_train.csv\")\n",
    "ts_stacking = pd.read_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_stacking['per_imp1']=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = lgb.Dataset(X_train.iloc[:500000], label=y_train.iloc[:500000])\n",
    "val_data = lgb.Dataset(X_train.iloc[500000:], label=y_train.iloc[500000:])\n",
    "\n",
    "clf = lgb.train(xgb_params, trn_data, int(c),fobj=rmsle,feval = rmsle_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking['per_imp1'] = clf.predict(X_train.iloc[500000:], num_iteration=clf.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_train.csv\",index=False)\n",
    "ts_stacking.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tr_stacking\n",
    "del ts_stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) 큰 것들 빼는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = mustin_columns+lag2_columns+lag288_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = real_train[column_list]\n",
    "X_test = real_test[column_list]\n",
    "\n",
    "y_train = real_train['proton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[real_train['proton']<=2000].reset_index().drop(['index'],axis=1)\n",
    "y_train = y_train.loc[real_train['proton']<=2000].reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB 0--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.122609\tvalid_1's myloss: 0.127126\n",
      "[1000]\ttraining's myloss: 0.112584\tvalid_1's myloss: 0.118936\n",
      "[1500]\ttraining's myloss: 0.109163\tvalid_1's myloss: 0.117751\n",
      "Early stopping, best iteration is:\n",
      "[1417]\ttraining's myloss: 0.109628\tvalid_1's myloss: 0.117375\n",
      "LGB 1--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.123417\tvalid_1's myloss: 0.12784\n",
      "[1000]\ttraining's myloss: 0.118772\tvalid_1's myloss: 0.124852\n",
      "Early stopping, best iteration is:\n",
      "[798]\ttraining's myloss: 0.119063\tvalid_1's myloss: 0.124141\n",
      "LGB 2--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.116595\tvalid_1's myloss: 0.122054\n",
      "[1000]\ttraining's myloss: 0.108216\tvalid_1's myloss: 0.116452\n",
      "Early stopping, best iteration is:\n",
      "[947]\ttraining's myloss: 0.1077\tvalid_1's myloss: 0.115472\n",
      "LGB 3--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.118765\tvalid_1's myloss: 0.123988\n",
      "[1000]\ttraining's myloss: 0.109896\tvalid_1's myloss: 0.118988\n",
      "Early stopping, best iteration is:\n",
      "[856]\ttraining's myloss: 0.109827\tvalid_1's myloss: 0.118228\n",
      "LGB 4--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.119271\tvalid_1's myloss: 0.127463\n",
      "[1000]\ttraining's myloss: 0.1137\tvalid_1's myloss: 0.124652\n",
      "[1500]\ttraining's myloss: 0.1114\tvalid_1's myloss: 0.125211\n",
      "Early stopping, best iteration is:\n",
      "[1268]\ttraining's myloss: 0.109394\tvalid_1's myloss: 0.122332\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "xgb_params={\"objective\":\"regression\",\n",
    "           \"metric\":\"myloss\",\n",
    "           \"max_depth\":6,\n",
    "           \"min_child_samples\":2,\n",
    "           \"alpha\":0.08,\n",
    "           \"gamma\":0.06,\n",
    "           \"eta\":0.04,\n",
    "           \"subsample\":0.08,\n",
    "           \"colsample_bytree\":0.97,\n",
    "           \"random_state\":2020\n",
    "           }\n",
    "a,b,c=get_oof_lgbm(xgb_params, X_train, X_test, y_train, num_round=100000, early_round=400, verbose_round=500, N_SPLITS=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking = pd.read_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_train.csv\")\n",
    "ts_stacking = pd.read_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_stacking['big_out2000']=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = lgb.Dataset(X_train.iloc[:500000].loc[real_train['proton'].iloc[:500000]<=2000].reset_index().drop(['index'],axis=1),\n",
    "                       label=y_train.iloc[:500000].loc[real_train['proton'].iloc[:500000]<=2000].reset_index().drop(['index'],axis=1))\n",
    "val_data = lgb.Dataset(X_train.iloc[500000:], label=y_train.iloc[500000:])\n",
    "\n",
    "clf = lgb.train(xgb_params, trn_data, int(c),fobj=rmsle,feval = rmsle_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = real_train[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_stacking['big_out2000'] = clf.predict(X_train.iloc[500000:], num_iteration=clf.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_train.csv\",index=False)\n",
    "ts_stacking.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_proton['proton'] = (ts_stacking['featureset_1']+ts_stacking['featureset_2']+ts_stacking['featureset_3']+ts_stacking['per_imp1']+ts_stacking['big_out2000'])/5\n",
    "\n",
    "# test_proton['proton'] = np.where(test_proton['proton']<0,0,test_proton['proton'])\n",
    "\n",
    "# test_proton.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/5_mean.csv\",index=False)\n",
    "\n",
    "# from aifactory.modules import activate, submit\n",
    "# submit(19, '/tf/notebooks/19th_problem/산식1.ipynb', '/tf/notebooks/19th_problem/lightgbm_model.zip', '/tf/notebooks/19th_problem/final_submission_files/5_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ts_stacking\n",
    "del tr_stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) orignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag2_lag1day_time_col_perimp = ['P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'xl_mean',\n",
    " 'H_DENSITY_#/cc_lens',\n",
    " 'SW_H_SPEED_km/s_mean',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag1',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'H_DENSITY_#/cc_lens_lag1',\n",
    " 'SW_H_SPEED_km/s_mean_lag1',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag288',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'xl_mean_lag288',\n",
    " 'H_DENSITY_#/cc_lens_lag288',\n",
    " 'SW_H_SPEED_km/s_mean_lag288',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag2',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'xl_mean_lag2',\n",
    " 'H_DENSITY_#/cc_lens_lag2',\n",
    " 'H_DENSITY_#/cc_mean_lag2',\n",
    " 'SW_H_SPEED_km/s_mean_lag2',\n",
    " 'time']\n",
    "\n",
    "lag2_lag1day_time_col = ['P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'xs_mean',\n",
    " 'xl_mean',\n",
    " 'H_DENSITY_#/cc_lens',\n",
    " 'H_DENSITY_#/cc_mean',\n",
    " 'SW_H_SPEED_km/s_mean',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag1',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'xs_mean_lag1',\n",
    " 'xl_mean_lag1',\n",
    " 'H_DENSITY_#/cc_lens_lag1',\n",
    " 'H_DENSITY_#/cc_mean_lag1',\n",
    " 'SW_H_SPEED_km/s_mean_lag1',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag288',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'xs_mean_lag288',\n",
    " 'xl_mean_lag288',\n",
    " 'H_DENSITY_#/cc_lens_lag288',\n",
    " 'H_DENSITY_#/cc_mean_lag288',\n",
    " 'SW_H_SPEED_km/s_mean_lag288',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag2',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'xs_mean_lag2',\n",
    " 'xl_mean_lag2',\n",
    " 'H_DENSITY_#/cc_lens_lag2',\n",
    " 'H_DENSITY_#/cc_mean_lag2',\n",
    " 'SW_H_SPEED_km/s_mean_lag2',\n",
    " 'time']\n",
    "\n",
    "lag2_lag1day_time_lagminus1day_col = ['P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean',\n",
    " 'xs_mean',\n",
    " 'xl_mean',\n",
    " 'H_DENSITY_#/cc_lens',\n",
    " 'H_DENSITY_#/cc_mean',\n",
    " 'SW_H_SPEED_km/s_mean',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag1',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1',\n",
    " 'xs_mean_lag1',\n",
    " 'xl_mean_lag1',\n",
    " 'H_DENSITY_#/cc_lens_lag1',\n",
    " 'H_DENSITY_#/cc_mean_lag1',\n",
    " 'SW_H_SPEED_km/s_mean_lag1',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag288',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288',\n",
    " 'xs_mean_lag288',\n",
    " 'xl_mean_lag288',\n",
    " 'H_DENSITY_#/cc_lens_lag288',\n",
    " 'H_DENSITY_#/cc_mean_lag288',\n",
    " 'SW_H_SPEED_km/s_mean_lag288',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag2',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2',\n",
    " 'xs_mean_lag2',\n",
    " 'xl_mean_lag2',\n",
    " 'H_DENSITY_#/cc_lens_lag2',\n",
    " 'H_DENSITY_#/cc_mean_lag2',\n",
    " 'SW_H_SPEED_km/s_mean_lag2',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag-288',\n",
    " 'P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag-288',\n",
    " 'xs_mean_lag-288',\n",
    " 'xl_mean_lag-288',\n",
    " 'H_DENSITY_#/cc_lens_lag-288',\n",
    " 'H_DENSITY_#/cc_mean_lag-288',\n",
    " 'SW_H_SPEED_km/s_mean_lag-288',\n",
    " 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_var = [lag2_lag1day_time_col_perimp,lag2_lag1day_time_col,lag2_lag1day_time_lagminus1day_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(selected_var)):\n",
    "    selected_var = selected_var[i]\n",
    "    X_train = real_train[selected_var]\n",
    "    X_test = real_test[selected_var]\n",
    "    y_train = real_train['proton']\n",
    "\n",
    "    a,b,c=get_oof_lgbm(xgb_params, X_train, X_test, y_train, num_round=100000, early_round=400, verbose_round=500, N_SPLITS=5, random_state=0)\n",
    "\n",
    "    trn_data = lgb.Dataset(X_train.iloc[:500000], label=y_train.iloc[:500000])\n",
    "    val_data = lgb.Dataset(X_train.iloc[500000:], label=y_train.iloc[500000:])\n",
    "\n",
    "    clf = lgb.train(xgb_params, trn_data, int(c),fobj=rmsle,feval = rmsle_eval)\n",
    "    agg_test[i] = b\n",
    "    agg_train[i] = clf.predict(X_train.iloc[500000:], num_iteration=clf.best_iteration)\n",
    "    print(np.mean(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final) ridge_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking = pd.read_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_train.csv\")\n",
    "ts_stacking = pd.read_csv(\"/tf/notebooks/19th_problem/final_submission_files/stacking_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [0.2,0,0.2,0,0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proton['proton'] = weight[0]*ts_stacking['featureset_1']+weight[1]*ts_stacking['featureset_2']+weight[2]*ts_stacking['featureset_3']+weight[3]*ts_stacking['per_imp1']+weight[4]*ts_stacking['big_out2000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proton['proton'] = np.where(test_proton['proton']<0,0,test_proton['proton'])\n",
    "\n",
    "test_proton.to_csv(\"/tf/notebooks/19th_problem/final_submission_files/5_weigted_mean20206.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.7MB [00:00, 51.5MB/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료 (시각: 2020-06-22 20:30:17.411009): 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from aifactory.modules import activate, submit\n",
    "submit(19, '/tf/notebooks/19th_problem/산식1.ipynb', '/tf/notebooks/19th_problem/lightgbm_model.zip', '/tf/notebooks/19th_problem/final_submission_files/5_weigted_mean20206.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('myloss', 0.2037714510622045, False)\n",
      "('myloss', 0.23791584213371628, False)\n",
      "('myloss', 0.20690735955682354, False)\n",
      "('myloss', 0.25159800812599087, False)\n",
      "('myloss', 0.19405866569857635, False)\n"
     ]
    }
   ],
   "source": [
    "for i in ['featureset_1', 'featureset_2', 'featureset_3', 'per_imp1',\n",
    "       'big_out2000']:\n",
    "    print(rmsle_eval1(tr_stacking[i],tr_stacking['proton']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.18229087950824288, False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle_eval1(pred,tr_stacking['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['featureset_1', 'featureset_2', 'featureset_3', 'proton', 'per_imp1',\n",
       "       'big_out2000'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_stacking.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tr_stacking[['featureset_1', 'featureset_2', 'featureset_3', 'per_imp1',\n",
    "       'big_out2000']]\n",
    "y_train = tr_stacking['proton']\n",
    "\n",
    "X_test = ts_stacking[['featureset_1', 'featureset_2', 'featureset_3', 'per_imp1',\n",
    "       'big_out2000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.79771159, -2.07153666, -1.98054242, ..., -1.65785025,\n",
       "       -1.6063802 , -1.70603065])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.19433199134786605, False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle_eval1((tr_stacking['featureset_1']+tr_stacking['featureset_2']+tr_stacking['featureset_3']+tr_stacking['per_imp1']+tr_stacking['big_out2000'])/5,tr_stacking['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.2037714510622045, False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle_eval1(tr_stacking['featureset_1'],tr_stacking['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.23791584213371628, False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle_eval1(tr_stacking['featureset_2'],tr_stacking['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.20690735955682354, False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle_eval1(tr_stacking['featureset_3'],tr_stacking['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.2515980081259908, False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle_eval1(tr_stacking['per_imp1'],tr_stacking['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.19405866569857635, False)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle_eval1(tr_stacking['big_out2000'],tr_stacking['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) lag1_proton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = real_train[[x for x in real_train.columns if x not in ['time_tag','proton']]]\n",
    "X_test = real_test[[x for x in real_test.columns if x not in ['time_tag','proton']]]\n",
    "\n",
    "y_train = real_train['proton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_eval(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    y = np.where(y<0,0,y)\n",
    "    \n",
    "    y_hat = pd.Series(y_hat)\n",
    "    y_hat = np.where(y_hat<0,0,y_hat)\n",
    "\n",
    "    return 'myloss', np.mean((np.log(y_hat+1)-np.log(y+1))**2), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    y = np.where(y<0,0,y)\n",
    "    \n",
    "    y_hat = pd.Series(y_hat)\n",
    "    y_hat = np.where(y_hat<0,0,y_hat)\n",
    "    grad = (1/(y_hat+1))*(np.log(y_hat+1)-np.log(y+1))\n",
    "    hess = (1/(y_hat+1))**2-(np.log(y_hat+1)-np.log(y+1))/(y_hat+1)**2\n",
    "    \n",
    "#     grad = pd.Series(grad)\n",
    "#     hess = pd.Series(hess)\n",
    "\n",
    "#     grad = np.where(y<10,grad,np.where(y<100,grad*100,np.where(y<1000,grad*200,np.where(y<10000,grad*1000,grad*6000))))\n",
    "#     hess = np.where(y<10,hess,np.where(y<100,hess*100,np.where(y<1000,hess*200,np.where(y<10000,hess*1000,hess*6000))))\n",
    "\n",
    "    return grad,hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof_lgbm(params, train_data, test_data, target_data, num_round, early_round, verbose_round, N_SPLITS=5, random_state=0):\n",
    "\n",
    "    FOLDs=KFold(n_splits=N_SPLITS, shuffle=True,random_state=0)\n",
    "\n",
    "    oof = np.zeros(len(train_data))\n",
    "    predictions = np.zeros(len(test_data))\n",
    "\n",
    "#     features_lgb = list(train_data.columns)\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train_data)):\n",
    "        trn_data = lgb.Dataset(train_data.iloc[trn_idx], label=target_data.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train_data.iloc[val_idx], label=target_data.iloc[val_idx])\n",
    "\n",
    "        print(\"LGB \" + str(fold_) + \"-\" * 50)\n",
    "        num_round = num_round\n",
    "        clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=verbose_round,\n",
    "                        fobj=rmsle,\n",
    "                        feval = rmsle_eval,\n",
    "                        early_stopping_rounds = early_round)\n",
    "        oof[val_idx] = clf.predict(train_data.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "#         fold_importance_df = pd.DataFrame()\n",
    "#         fold_importance_df[\"feature\"] = features_lgb\n",
    "#         fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#         fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        predictions += clf.predict(test_data, num_iteration=clf.best_iteration) / FOLDs.n_splits\n",
    "        joblib.dump(clf,'lgb'+str(fold_)+'.pkl')\n",
    "    return oof, predictions, feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/tf/notebooks/19th_problem/lightgbm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB 0--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.129578\tvalid_1's myloss: 0.133294\n",
      "[1000]\ttraining's myloss: 0.112044\tvalid_1's myloss: 0.118906\n",
      "[1500]\ttraining's myloss: 0.108441\tvalid_1's myloss: 0.118753\n",
      "Early stopping, best iteration is:\n",
      "[1269]\ttraining's myloss: 0.109246\tvalid_1's myloss: 0.117773\n",
      "LGB 1--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.130695\tvalid_1's myloss: 0.134124\n",
      "[1000]\ttraining's myloss: 0.11843\tvalid_1's myloss: 0.126736\n",
      "[1500]\ttraining's myloss: 0.114594\tvalid_1's myloss: 0.125752\n",
      "Early stopping, best iteration is:\n",
      "[1252]\ttraining's myloss: 0.115024\tvalid_1's myloss: 0.124557\n",
      "LGB 2--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.131897\tvalid_1's myloss: 0.13946\n",
      "[1000]\ttraining's myloss: 0.1238\tvalid_1's myloss: 0.133817\n",
      "[1500]\ttraining's myloss: 0.120771\tvalid_1's myloss: 0.134014\n",
      "Early stopping, best iteration is:\n",
      "[1219]\ttraining's myloss: 0.122288\tvalid_1's myloss: 0.133056\n",
      "LGB 3--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.125072\tvalid_1's myloss: 0.128067\n",
      "[1000]\ttraining's myloss: 0.111487\tvalid_1's myloss: 0.118094\n",
      "[1500]\ttraining's myloss: 0.106304\tvalid_1's myloss: 0.116402\n",
      "Early stopping, best iteration is:\n",
      "[1243]\ttraining's myloss: 0.107166\tvalid_1's myloss: 0.115257\n",
      "LGB 4--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.127185\tvalid_1's myloss: 0.133342\n",
      "[1000]\ttraining's myloss: 0.12058\tvalid_1's myloss: 0.129696\n",
      "[1500]\ttraining's myloss: 0.11841\tvalid_1's myloss: 0.130387\n",
      "Early stopping, best iteration is:\n",
      "[1144]\ttraining's myloss: 0.119611\tvalid_1's myloss: 0.129359\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "xgb_params={\"objective\":\"regression\",\n",
    "           \"metric\":\"myloss\",\n",
    "           \"max_depth\":6,\n",
    "           \"min_child_samples\":2,\n",
    "           \"alpha\":0.08,\n",
    "           \"gamma\":0.06,\n",
    "           \"eta\":0.04,\n",
    "           \"subsample\":0.08,\n",
    "           \"colsample_bytree\":0.97,\n",
    "           \"random_state\":2020\n",
    "           }\n",
    "a,b,c=get_oof_lgbm(xgb_params, X_train, X_test, y_train, num_round=100000, early_round=400, verbose_round=500, N_SPLITS=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proton['proton'] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proton['proton'] = np.where(test_proton['proton']<0,0,test_proton['proton'])\n",
    "\n",
    "test_proton.to_csv(\"../eval_lag2_lag1day.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/notebooks/19th_problem/lightgbm_model.zip'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "zip_name = 'lightgbm_model'\n",
    "directory_name = 'lightgbm_model'\n",
    "# Create 'path\\to\\zip_file.zip'\n",
    "shutil.make_archive(zip_name, 'zip', directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3.20M/21.7M [00:00<00:00, 33.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.7MB [00:00, 65.5MB/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료 (시각: 2020-06-22 04:57:26.856196): 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from aifactory.modules import activate, submit\n",
    "submit(19, '산식1.ipynb', 'lightgbm_model.zip', 'eval_lag2_lag1day.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799488"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params={\"objective\":\"regression\",\n",
    "           \"metric\":\"myloss\",\n",
    "           \"max_depth\":6,\n",
    "           \"min_child_samples\":2,\n",
    "           \"alpha\":0.08,\n",
    "           \"gamma\":0.06,\n",
    "           \"eta\":0.04,\n",
    "           \"subsample\":0.08,\n",
    "           \"colsample_bytree\":0.97,\n",
    "           \"random_state\":2020\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\ttraining's myloss: 0.132788\tvalid_1's myloss: 0.0594058\n",
      "Early stopping, best iteration is:\n",
      "[247]\ttraining's myloss: 0.159696\tvalid_1's myloss: 0.0534542\n"
     ]
    }
   ],
   "source": [
    "trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "clf = lgb.train(xgb_params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=500,\n",
    "                fobj=rmsle,\n",
    "                feval = rmsle_eval,\n",
    "                early_stopping_rounds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proton['proton'] = clf.predict(X_test, num_iteration=clf.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proton['proton'] = np.where(test_proton['proton']<0,0,test_proton['proton'])\n",
    "\n",
    "test_proton.to_csv(\"../eval_lag2_lag1day_time_validationset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3.98M/21.7M [00:00<00:00, 41.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.7MB [00:00, 69.6MB/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료 (시각: 2020-06-22 05:55:46.193671): 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from aifactory.modules import activate, submit\n",
    "submit(19, '산식1.ipynb', 'lightgbm_model.zip', 'eval_lag2_lag1day_time_validationset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\ttraining's myloss: 0.115025\tvalid_1's myloss: 0.176178\n",
      "Early stopping, best iteration is:\n",
      "[262]\ttraining's myloss: 0.153925\tvalid_1's myloss: 0.144744\n"
     ]
    }
   ],
   "source": [
    "trn_data = lgb.Dataset(X_train.iloc[:500000], label=y_train.iloc[:500000])\n",
    "val_data = lgb.Dataset(X_train.iloc[500000:], label=y_train.iloc[500000:])\n",
    "\n",
    "clf = lgb.train(xgb_params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=500,\n",
    "                fobj=rmsle,\n",
    "                feval = rmsle_eval,\n",
    "                early_stopping_rounds = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_eval(y_hat, y):\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    y = np.where(y<0,0,y)\n",
    "    \n",
    "    y_hat = pd.Series(y_hat)\n",
    "    y_hat = np.where(y_hat<0,0,y_hat)\n",
    "\n",
    "    return 'myloss', np.mean((np.log(y_hat+1)-np.log(y+1))**2), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
      "xs_mean\n",
      "xl_mean\n",
      "H_DENSITY_#/cc_lens\n",
      "H_DENSITY_#/cc_mean\n",
      "SW_H_SPEED_km/s_mean\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag1\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag1\n",
      "xs_mean_lag1\n",
      "xl_mean_lag1\n",
      "H_DENSITY_#/cc_lens_lag1\n",
      "H_DENSITY_#/cc_mean_lag1\n",
      "SW_H_SPEED_km/s_mean_lag1\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag288\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag288\n",
      "xs_mean_lag288\n",
      "xl_mean_lag288\n",
      "H_DENSITY_#/cc_lens_lag288\n",
      "H_DENSITY_#/cc_mean_lag288\n",
      "SW_H_SPEED_km/s_mean_lag288\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_lag2\n",
      "P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P3P_.114-.190MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P6P_.580-1.05MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P7P_1.05-1.89MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_lag2\n",
      "xs_mean_lag2\n",
      "xl_mean_lag2\n",
      "H_DENSITY_#/cc_lens_lag2\n",
      "H_DENSITY_#/cc_mean_lag2\n",
      "SW_H_SPEED_km/s_mean_lag2\n",
      "time\n"
     ]
    }
   ],
   "source": [
    "rmsle_list=[]\n",
    "for column in X_train.columns:\n",
    "    X_train_fake = X_train.iloc[500000:].copy()\n",
    "    X_train_fake[column] = X_train_fake[column].sample(len(X_train_fake),replace=False,random_state=2020).reset_index(drop=True)\n",
    "    rmsle_list.append(rmsle_eval(clf.predict(X_train_fake, num_iteration=clf.best_iteration),y_train.iloc[500000:]))\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myloss', 0.14474437579789878, False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fake = X_train.iloc[500000:].copy()\n",
    "rmsle_eval(clf.predict(X_train_fake, num_iteration=clf.best_iteration),y_train.iloc[500000:])\n",
    "\n",
    "imp = pd.DataFrame({'rmsle':[x[1] for x in rmsle_list],'col_list':X_train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp['rmsle'] = imp['rmsle']-0.14474437579789878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1124ed409b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rmsle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'col_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imp' is not defined"
     ]
    }
   ],
   "source": [
    "imp.loc[imp['rmsle']<=0]['col_list'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmsle</th>\n",
       "      <th>col_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.009340</td>\n",
       "      <td>P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.005038</td>\n",
       "      <td>P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003498</td>\n",
       "      <td>P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.001724</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.001424</td>\n",
       "      <td>xl_mean_lag1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.001129</td>\n",
       "      <td>P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.000997</td>\n",
       "      <td>ratio_mean_lag288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.000375</td>\n",
       "      <td>xl_mean_lag288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.000339</td>\n",
       "      <td>xl_mean_lag2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.000239</td>\n",
       "      <td>P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.000212</td>\n",
       "      <td>P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.000207</td>\n",
       "      <td>xl_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.000206</td>\n",
       "      <td>SW_H_SPEED_km/s_mean_lag1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.000062</td>\n",
       "      <td>xs_mean_lag2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>H_DENSITY_#/cc_lens_lag288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>H_DENSITY_#/cc_lens_lag1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>H_DENSITY_#/cc_lens_lag2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>H_DENSITY_#/cc_lens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>H_DENSITY_#/cc_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>H_DENSITY_#/cc_mean_lag1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000047</td>\n",
       "      <td>P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.000068</td>\n",
       "      <td>ratio_mean_lag2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>ratio_mean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rmsle                                           col_list\n",
       "24 -0.009340  P8P_1.89-4.75MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "5  -0.005038      P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
       "4  -0.003498      P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean\n",
       "64 -0.001724                                               time\n",
       "26 -0.001424                                       xl_mean_lag1\n",
       "50 -0.001129  P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "43 -0.000997                                  ratio_mean_lag288\n",
       "42 -0.000375                                     xl_mean_lag288\n",
       "58 -0.000339                                       xl_mean_lag2\n",
       "52 -0.000239  P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "18 -0.000212  P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "10 -0.000207                                            xl_mean\n",
       "31 -0.000206                          SW_H_SPEED_km/s_mean_lag1\n",
       "57 -0.000062                                       xs_mean_lag2\n",
       "45  0.000000                         H_DENSITY_#/cc_lens_lag288\n",
       "29  0.000000                           H_DENSITY_#/cc_lens_lag1\n",
       "61  0.000000                           H_DENSITY_#/cc_lens_lag2\n",
       "48  0.000000  P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_...\n",
       "0   0.000000      P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens\n",
       "32  0.000000  P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_...\n",
       "16  0.000000  P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_lens_...\n",
       "13  0.000000                                H_DENSITY_#/cc_lens\n",
       "49  0.000006  P1P_.047-.066MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "14  0.000009                                H_DENSITY_#/cc_mean\n",
       "34  0.000014  P2P_.066-.114MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "37  0.000030  P5P_.310-.580MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "30  0.000036                           H_DENSITY_#/cc_mean_lag1\n",
       "36  0.000047  P4P_.190-.310MEV_IONS_1/(cm**2-s-sr-MeV)_mean_...\n",
       "59  0.000068                                    ratio_mean_lag2\n",
       "11  0.000091                                         ratio_mean"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.sort_values(['rmsle']).iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof[val_idx] = clf.predict(train_data.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "predictions += clf.predict(test_data, num_iteration=clf.best_iteration) / FOLDs.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lag변수 proton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train['proton_pred'] = a\n",
    "real_test['proton_pred'] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train.to_csv(\"../real_train_with_pred.csv\",index=False)\n",
    "real_test.to_csv(\"../real_test_with_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train = pd.read_csv(\"../real_train_with_pred.csv\")\n",
    "real_test = pd.read_csv(\"../real_test_with_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train = real_train.set_index('time_tag')\n",
    "\n",
    "shift_df = real_train[['proton_pred']].shift(1)\n",
    "\n",
    "shift_df = shift_df.rename(columns = {'proton_pred':'proton_pred_lag1'})\n",
    "    \n",
    "real_train = pd.merge(real_train,shift_df.reset_index(),on='time_tag',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_test = real_test.set_index('time_tag')\n",
    "\n",
    "shift_df = real_test[['proton_pred']].shift(1)\n",
    "\n",
    "shift_df = shift_df.rename(columns={'proton_pred':'proton_pred_lag1'})\n",
    "    \n",
    "real_test = pd.merge(real_test,shift_df.reset_index(),on='time_tag',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "del real_train['proton_pred']\n",
    "del real_test['proton_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = real_train[[x for x in real_train.columns if x not in ['time_tag','proton']]]\n",
    "X_test = real_test[[x for x in real_test.columns if x not in ['time_tag','proton']]]\n",
    "\n",
    "y_train = real_train['proton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7568272912313208"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_train['proton_pred_lag1'].corr(real_train['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB 0--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.0907622\tvalid_1's myloss: 0.0951908\n",
      "[1000]\ttraining's myloss: 0.085191\tvalid_1's myloss: 0.0937214\n",
      "Early stopping, best iteration is:\n",
      "[946]\ttraining's myloss: 0.0854912\tvalid_1's myloss: 0.0933648\n",
      "LGB 1--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.0891628\tvalid_1's myloss: 0.0935346\n",
      "[1000]\ttraining's myloss: 0.085707\tvalid_1's myloss: 0.0935802\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's myloss: 0.0874615\tvalid_1's myloss: 0.093183\n",
      "LGB 2--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.0892055\tvalid_1's myloss: 0.0991674\n",
      "Early stopping, best iteration is:\n",
      "[564]\ttraining's myloss: 0.088263\tvalid_1's myloss: 0.098645\n",
      "LGB 3--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.0873973\tvalid_1's myloss: 0.0921942\n",
      "[1000]\ttraining's myloss: 0.0810111\tvalid_1's myloss: 0.0891614\n",
      "Early stopping, best iteration is:\n",
      "[929]\ttraining's myloss: 0.0814728\tvalid_1's myloss: 0.088891\n",
      "LGB 4--------------------------------------------------\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[500]\ttraining's myloss: 0.0924485\tvalid_1's myloss: 0.100819\n",
      "Early stopping, best iteration is:\n",
      "[492]\ttraining's myloss: 0.0921572\tvalid_1's myloss: 0.100719\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "xgb_params={\"objective\":\"regression\",\n",
    "           \"metric\":\"myloss\",\n",
    "           \"max_depth\":6,\n",
    "           \"min_child_samples\":2,\n",
    "           \"alpha\":0.08,\n",
    "           \"gamma\":0.06,\n",
    "           \"eta\":0.04,\n",
    "           \"subsample\":0.08,\n",
    "           \"colsample_bytree\":0.97,\n",
    "           \"random_state\":2020\n",
    "           }\n",
    "a,b,c=get_oof_lgbm(xgb_params, X_train, X_test, y_train, num_round=100000, early_round=400, verbose_round=500, N_SPLITS=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proton['proton'] = b\n",
    "\n",
    "test_proton['proton'] = np.where(test_proton['proton']<0,0,test_proton['proton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proton.to_csv(\"eval_lag2_lag1day_time_xray_lag1proton.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./19th_problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/21.7M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.7MB [00:00, 28.1MB/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료 (시각: 2020-06-21 22:22:23.687195): 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from aifactory.modules import activate, submit\n",
    "submit(19, '산식1.ipynb', 'lightgbm_model.zip', 'eval_lag2_lag1day_time_xray_lag1proton.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. time series cross validation\n",
    "# 2. 산식 최적화(최적화2, 최적화1, 아무것도 안함(rmse))\n",
    "# 3. 변수선택"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
